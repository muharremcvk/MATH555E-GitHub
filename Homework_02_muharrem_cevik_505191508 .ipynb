{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "There are four text files in the data folder\n",
    "\n",
    "* Atat√ºrk's \"Nutuk\" in Turkish\n",
    "* Dicken's novel \"Great Expectations\" in English\n",
    "* Flauberts' novel \"Madam Bovary\" in French\n",
    "* A text file `unknown.txt` in an unknown language\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Calculate how many times each character (letter) appear in each text.\n",
    "* Calculate the character distributions, i.e. using the character counts, calculate the probability of each character appearing in the text.\n",
    "* Find the set of characters common to all three texts.\n",
    "* Using the common set and the KL-divergence, show that each language have different character distributions.\n",
    "* Determine the language of the text file `unknown.txt` KL-divergence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from re import sub\n",
    "import re\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "nutuk = open('ataturk_nutuk.txt', 'r', encoding='utf8')\n",
    "text1 = nutuk.read().lower()\n",
    "processed1 = sub('[^a-z ]','',text1).split()\n",
    "\n",
    "res1= Counter(str(processed1))\n",
    "\n",
    "g_expectations = open('dickens_great_expectations.txt', 'r', encoding='utf8')\n",
    "text2 = g_expectations.read().lower()\n",
    "processed2 = sub('[^a-z ]','',text2).split()\n",
    "\n",
    "res2= Counter(str(processed2))\n",
    "\n",
    "flaubert_madame_bovary = open('flaubert_madame_bovary.txt', 'r', encoding='utf8')\n",
    "text3 = flaubert_madame_bovary.read().lower()\n",
    "processed3 = sub('[^a-z ]','',text3).split()\n",
    "\n",
    "res3= Counter(str(processed3))\n",
    "\n",
    "#I am having a problem finding the probability of each character and I could not solve this problem.\n",
    "#If we call these probability distributions P[], Q[] and R[], the solution to the problem is as follows;\n",
    "#mc1= Counter(P).most_common(5)\n",
    "#mc2= Counter(Q).most_common(5)\n",
    "#mc3= Counter(R).most_common(5)\n",
    "\n",
    "#KL_PQ = rel_entr(P, Q)\n",
    "#print('KL(P || Q): %.3f' % sum(KL_PQ))\n",
    "\n",
    "#KL_PR = rel_entr(P, R)\n",
    "#print('KL(P || R): %.3f' % sum(KL_PR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question consider the [Car Evaluation Data Set](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data) to the dataset.\n",
    "\n",
    "Make [contingency tables](https://en.wikipedia.org/wiki/Contingency_table#:~:text=In%20statistics%2C%20a%20contingency%20table,%2C%20engineering%2C%20and%20scientific%20research.) of the columns (using [`crosstab`](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) function from [pandas](https://pandas.pydata.org)) and figure out which pairs of columns are dependent and independent. Explain your result using statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bioinfokit.analys import stat\n",
    "data=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\", names= [\"Buying\", \"Maint\", \"Doors\", \"Persons\", \"Lug_boot\", \"Safety\", \"Class\"])\n",
    "ctab1= pd.crosstab(data.Buying, data.Class) #H0= There is no relation between buying and class.\n",
    "#print(ctab1)                                #Because the P-value is too low (P= 5.9e-36<0.05), we can reject the null hypothesis. \n",
    "res1= stat()                                #So we can say that this pairs of columns are strongly dependent. \n",
    "res1.chisq(df= ctab1)\n",
    "#print(res1.expected_df) #expected values\n",
    "#print(res1.summary)\n",
    "\n",
    "ctab2= pd.crosstab(data.Maint, data.Class)  #H0= There is no relation between maint and class.\n",
    "#print(ctab2)                                #Because the P-value is too low (P= 2.5e-26<0.05), we can reject the null hypothesis.\n",
    "res2= stat()                                #So we can say that this pairs of columns are strongly dependent. \n",
    "res2.chisq(df= ctab2)\n",
    "#print(res2.summary)\n",
    "\n",
    "ctab3= pd.crosstab(data.Doors, data.Class)  #H0= There is no relation between doors and class.\n",
    "#print(ctab3)                                #Because of P= 0.32>0.05, we cannot reject the null hypothesis.\n",
    "res3= stat()                                #So we can say that this pairs of columns are independent\n",
    "res3.chisq(df= ctab3)\n",
    "#print(res3.summary)\n",
    "\n",
    "ctab4= pd.crosstab(data.Persons, data.Class)  #H0= There is no relation between persons and class.\n",
    "#print(ctab4)                                 #Because the P-value is too low (P= 4e-77<0.05), we can reject the null hypothesis.\n",
    "res4= stat()                                 #So we can say that this pairs of columns are strongly dependent.\n",
    "res4.chisq(df= ctab4)\n",
    "#print(res4.summary)\n",
    "\n",
    "ctab5= pd.crosstab(data.Lug_boot, data.Class)  #H0= There is no relation between lug boot and class.\n",
    "#print(ctab5)                                 #Because the P-value is too low (P= 1e-9<0.05), we can reject the null hypothesis.\n",
    "res5= stat()                                 #So we can say that this pairs of columns are strongly dependent.\n",
    "res5.chisq(df= ctab5)\n",
    "#print(res5.summary)\n",
    "\n",
    "ctab6= pd.crosstab(data.Safety, data.Class)  #H0= There is no relation between safety and class.\n",
    "print(ctab6)                                 #Because the P-value is too low (P= 2.4e-100<0.05), we can reject the null hypothesis.\n",
    "res6= stat()                                 #So we can say that this pairs of columns are strongly dependent.\n",
    "res6.chisq(df= ctab6)\n",
    "print(res6.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #3\n",
    "\n",
    "For this question, use [Default of Credit Card Clients Data Set]() from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Inspect the dataset.\n",
    "* Would it be appropriate to form a linear regression model to predict the `default payment next month` variable? Explain.\n",
    "* Form a [contingency table](https://en.wikipedia.org/wiki/Contingency_table#:~:text=In%20statistics%2C%20a%20contingency%20table,%2C%20engineering%2C%20and%20scientific%20research.) of the columns `SEX` vs `default payment next month` and `EDUCATION` vs `default payment next month`.\n",
    "* Are there statistically verifiable relationships between credit card defaults, the gender of and the education level the borrower? Which is stronger? Quantify your analysis using [Chi Square Test](https://en.wikipedia.org/wiki/Chi-squared_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from bioinfokit.analys import stat\n",
    "data=pd.read_excel(\"default of credit card clients.xlsx\")\n",
    "data.dtypes\n",
    "y= np.array(data['default_payment_next_month'])\n",
    "X= np.array(data.drop('default_payment_next_month', axis=1))\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2, random_state=2)\n",
    "Linear= LinearRegression()\n",
    "Linear.fit(X_train, y_train)\n",
    "Linear.score(X_test, y_test)\n",
    "Linear.score(X_train, y_train)\n",
    "#In the linear regression model created, the accuracy rate of test data is approximately 13 percent \n",
    "#and the accuracy rate of train data is approximately 12.2 percent.\n",
    "#Therefore, the linear regression model cannot be used to predict the default payment next month variable.\n",
    "\n",
    "\n",
    "ctab1= pd.crosstab(data.SEX, data.default_payment_next_month)\n",
    "print(ctab1)\n",
    "res1= stat()\n",
    "res1.chisq(df= ctab1)\n",
    "print(res1.expected_df)\n",
    "print(res1.summary)        #P-value= 4.94468e-12\n",
    "\n",
    "ctab2= pd.crosstab(data.EDUCATION, data.default_payment_next_month)\n",
    "print(ctab2)\n",
    "res2= stat()\n",
    "res2.chisq(df= ctab2)\n",
    "print(res2.expected_df)\n",
    "print(res2.summary)        #P-value= 1.23326e-32\n",
    "\n",
    "#According to the results, there are statistically verifiable relationships between credit card defaults,\n",
    "#the gender of and the education level the borrower. I can clearly say that the relationship between \"EDUCATION\" \n",
    "#and \"default payment next month\" is stronger than the relationship between \"SEX\" and \"default payment next month\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #4\n",
    "\n",
    "For this question, use the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris) from UCI.  Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Form a [K-NN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) model for this dataset.\n",
    "* Test your model on random samples of your data and calculate its accuracy.\n",
    "* Repeat your calculation 100 times and give an interval of accuracy values leaving the best 2.5% and worst 2.5% accuracy values.\n",
    "* Is there a better way of doing this without repeating the calculation 100 times? Explain.\n",
    "* Find the best parameter $k$ for your dataset for the K-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mglearn\n",
    "\n",
    "dataset= pd.read_csv('iris.data', names=['sepal length', 'sepal width', 'petal length', 'petal width', 'class'])\n",
    "X= dataset.iloc[:,:4]\n",
    "y= dataset.iloc[:,-1]\n",
    "X= preprocessing.StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.3, random_state=1)\n",
    "train_accuracy=[]\n",
    "test_accuracy= []\n",
    "numberofK= range(1, 100)\n",
    "\n",
    "for N in numberofK:\n",
    "    clss= KNeighborsClassifier(n_neighbors= N)\n",
    "    clss.fit(X_train, y_train)\n",
    "    train_accuracy.append(clss.score(X_train, y_train))\n",
    "    test_accuracy.append(clss.score(X_test, y_test))\n",
    "plt.plot(numberofK, train_accuracy, label=' train accuracy')\n",
    "plt.plot(numberofK, test_accuracy, label='test accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('K-neighbors')\n",
    "plt.legend()\n",
    "\n",
    "#Without repeating the calculations 100 times, we can show the K-neighbors values \n",
    "#corresponding to the accuracy ratio on the graph in order to do the calculations in the best way.\n",
    "#Accordingly, the most suitable parameter k seems to be 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #5\n",
    "\n",
    "For this question, we are going to use [Concrete Slump Test Dataset](https://archive.ics.uci.edu/ml/datasets/Concrete+Slump+Test) from UCI. Here is the [direct link](https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data) to the dataset.\n",
    "\n",
    "Your tasks are\n",
    "\n",
    "* Form three separate linear regression model for the following dependent variables:\n",
    "\n",
    "  - SLUMP (cm)\n",
    "  - FLOW (cm)\n",
    "  - 28-day Compressive Strength (Mpa)\n",
    "  \n",
    "* Compare how well these models fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "data=pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/slump_test.data\")\n",
    "data.drop('No', axis=1, inplace= True)\n",
    "y1= np.array(data['SLUMP(cm)'])\n",
    "y2= np.array(data['FLOW(cm)'])\n",
    "y3= np.array(data['Compressive Strength (28-day)(Mpa)'])\n",
    "X= np.array(data.drop(['SLUMP(cm)', 'FLOW(cm)', 'Compressive Strength (28-day)(Mpa)'], axis=1))\n",
    "X_train, X_test, y1_train, y1_test= train_test_split(X,y1,test_size=0.3, random_state=3)\n",
    "Linear1= LinearRegression()\n",
    "Linear1.fit(X_train, y1_train)\n",
    "Linear1.score(X_test, y1_test)     \n",
    "Linear1.score(X_train, y1_train)   #= 0.378\n",
    "\n",
    "X_train, X_test, y2_train, y2_test= train_test_split(X,y2,test_size=0.2, random_state=2)\n",
    "Linear2= LinearRegression()\n",
    "Linear2.fit(X_train, y2_train)\n",
    "Linear2.score(X_test, y2_test)     \n",
    "Linear2.score(X_train, y2_train)   #= 0.528\n",
    "\n",
    "X_train, X_test, y3_train, y3_test= train_test_split(X,y3,test_size=0.2, random_state=2)\n",
    "Linear3= LinearRegression()\n",
    "Linear3.fit(X_train, y3_train)\n",
    "Linear3.score(X_test, y3_test)     \n",
    "Linear3.score(X_train, y3_train)   #= 0.906\n",
    "\n",
    "#Considering the train and test scores, the model depends on the 28-day Compressive Strength (Mpa) variable \n",
    "#gives the best result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
