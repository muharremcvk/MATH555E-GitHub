{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #1\n",
    "\n",
    "For this question you will use [Olivetti Face Dataset](https://scikit-learn.org/0.19/datasets/olivetti_faces.html).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "1. Split your dataset as train and test subset. But make sure that each test set contains exactly one random image from each distinct individual. This means, you will have to write your own train_test_split function for this dataset.\n",
    "\n",
    "2. Construct an SVM model on your train set, and test its accuracy on your test set. For this part, the images viewed as integer vectors of length 4096 are independent variables while the id number of the person that picture belongs to is the dependent variable. In other words, you are trying to construct an SVM model that recognizes individuals based on their pictures.\n",
    "\n",
    "3. Repeat Step 2 ten times.\n",
    "\n",
    "4. Calculate the mean accuracy and get 95% confidence interval using the t-test.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Do the same things you did in Part 1 but with a multinomial regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-1 Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datas= fetch_olivetti_faces()\n",
    "X= datas.data\n",
    "y= datas.target\n",
    "\n",
    "train_X, test_X, train_y, test_y= train_test_split(X, y, train_size= 0.8, test_size= 0.2, random_state=0)\n",
    "\n",
    "svm_model= SVC(kernel=\"linear\")\n",
    "svm_model.fit(train_X, train_y)\n",
    "\n",
    "\n",
    "svm_model.score(test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-1 Part 2 \n",
    "\n",
    "Here, The SVM model gives a better result than the multinomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "m_regression= LogisticRegression(max_iter=5000)\n",
    "m_regression.fit(train_X, train_y)\n",
    "\n",
    "m_regression.score(test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question #2\n",
    "\n",
    "For this question you will use [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "### Part 1\n",
    "\n",
    "Convert the dataset into numerical data using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) from SciKitLearn's `sklearn.feature_extraction.text` module. Make sure that you also record whether a given movie review is positive or negative or neutral. Calling on `CountVectorizer` on individual entries is not going to be enough. You will have to do some preliminary work. Read the documentation carefully.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "Using the numerical data you constructed in Part 1, construct an LDA model to see if data projects into a 2D space with clear separation. Analyze your result.\n",
    "\n",
    "\n",
    "### Part 3\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Split the data as train and test using SciKitLearn's `train_test_split` function.\n",
    "2. Form a multiclass SVM model on the train set and test its accuracy.\n",
    "3. Repeat a small number of times and get mean accuracy and its error band.\n",
    "\n",
    "### Part 4\n",
    "\n",
    "Repeat Part 2 using multinomial regression models instead of SVM.\n",
    "\n",
    "### Part 5\n",
    "\n",
    "Using the numerical data you constructed in Part 1, \n",
    "\n",
    "1. Construct an PCA model and look at the eigenvalues from largest to smallest. \n",
    "2. How many dimensions needed to capture 90% of the variation of the data? (Read the documentation of [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) form SciKitLearn)\n",
    "3. Transform your data using the result you obtained in Step 2.\n",
    "4. Construct an SVM model on the new dataset you constructed and cross-validate it.\n",
    "5. Compare your result with the result you obtained in Part 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-2 Part 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_r= []\n",
    "for line in open(r'C:\\Users\\muhar\\Desktop\\mat555e\\movie_data\\train.txt', encoding='utf8'):\n",
    "    train_r.append(line.strip())\n",
    "    \n",
    "test_r= []\n",
    "for line in open(r'C:\\Users\\muhar\\Desktop\\mat555e\\movie_data\\test.txt', encoding='utf8'):\n",
    "    test_r.append(line.strip())\n",
    "    \n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "train_reviews = preprocess_reviews(train_r)\n",
    "test_reviews = preprocess_reviews(test_r)\n",
    "\n",
    "vector= CountVectorizer()\n",
    "vector.fit(train_reviews)\n",
    "\n",
    "X = vector.transform(train_reviews)\n",
    "X_t= vector.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-2 Part 2\n",
    "\n",
    "I get this error; MemoryError: Unable to allocate 16.1 GiB for an array with shape (25000, 86374) and data type int64\n",
    "But I couldn't figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X_toarray= X.toarray()\n",
    "model_lna= LinearDiscriminantAnalysis()\n",
    "model_lna.fit(X_toarray,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-2 Part-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dec1032a8254>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_t\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vector' is not defined"
     ]
    }
   ],
   "source": [
    "X = vector.transform(train_reviews)\n",
    "X_t= vector.transform(test_reviews)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, target, train_size= 0.2, test_size= 0.05)\n",
    "\n",
    "svm_model= SVC(kernel=\"linear\")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_model.score(X_test, y_test)\n",
    "svm_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-2 Part-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model= LogisticRegression(max_iter=5000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_model.score(X_test, y_test)\n",
    "lr_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-2 Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse as sp\n",
    "\n",
    "\n",
    "scaler=StandardScaler(with_mean=False)\n",
    "scaler.fit(X)\n",
    "scaled_data= scaler.transform(X)\n",
    "\n",
    "pca_model= TruncatedSVD(100)\n",
    "pca_model.fit(scaled_data)\n",
    "x_pca_model= pca_model.transform(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. To show you exactly what I did, I wrote even if the code didn't work. \n",
    "Also, for your convenience, I split it up in parts but it gave a NameError error in some cells. \n",
    "For question 2, to check if the code is working, it is useful to remove part 2 and copy the whole code into the same cell \n",
    "and check. For your information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
